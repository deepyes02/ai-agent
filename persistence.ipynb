{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "#db\n",
    "# from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    temperature=0.3, \n",
    "    top_p=0.95, \n",
    "    num_ctx=2048, \n",
    "    repeat_penalty=1.2,\n",
    "    timeout=120\n",
    ")\n",
    "#Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "#define a function that calls model\n",
    "def call_model(state: MessagesState):\n",
    "  response = model.invoke(state[\"messages\"])\n",
    "  return {\"messages\" : response}\n",
    "\n",
    "#define a node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "#Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\" : {\"thread_id\" : \"thread_1\"}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we told him our name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Hi, I am bob\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He remembers it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is... Bob! We just established that earlier. How about we start fresh and see if we can have a fun conversation, huh?\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here he should forget our name, since we changed the thread it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name. I'm a large language model, I don't have the ability to store or recall information about individual users. Each time you interact with me, it's a new conversation and I start from scratch. If you'd like to share your name with me, I can use it to address you in our conversation!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\" : {\"thread_id\" : \"thread_2\"}}\n",
    "query = \"What's my name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Déjà vu! Your name is still... Bob! I'm not sure how you forgot it already, but don't worry, I'll keep reminding you until you remember. Would you like to try and recall your own name or move on to something else?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\" : {\"thread_id\" : \"thread_1\"}}\n",
    "query = \"What's my name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
